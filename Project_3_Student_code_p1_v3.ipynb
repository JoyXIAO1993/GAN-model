{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKRfNs6ICuLD"
   },
   "source": [
    "#Student Name: \n",
    "#ECE 595 Machine Learning II\n",
    "#Project 3: GAN - Student Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nfXEi7WC0OA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import necessary packages\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fACNZwo4DBrP"
   },
   "source": [
    "#Part 1: Implementing the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAKtkDn6DQsy",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bcc23f6a994c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Load MNIST data and normalize to [-1, 1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Fill this in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m127.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m127.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "#Load MNIST data and normalize to [-1, 1]\n",
    "# Fill this in\n",
    "(data_train, _), (_, _) = mnist.load_data()\n",
    "data_train = (data_train - 127.5)/127.5\n",
    "\n",
    "data_train = np.expand_dims(data_train, axis=3)\n",
    "\n",
    "# The D-dimensional noise vector length\n",
    "latent_dim = 100\n",
    "data_dim = 28 * 28 * 1\n",
    "\n",
    "img_shape=(28,28,1)\n",
    "\n",
    "# Optimizer for discriminator, which will have a higher learning rate than adversarial model\n",
    "def adam_optimizer():\n",
    "    # FILL THIS IN\n",
    "    return Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "def gan_optimizer():\n",
    "    return Adam(lr=0.001)\n",
    "\n",
    "\n",
    "# Genrerator model\n",
    "def create_generator():\n",
    "    # FILL THIS IN\n",
    "    generator=Sequential()\n",
    "    generator.add(Dense(256, input_dim=latent_dim))\n",
    "    generator.add(LeakyReLU(0.5))\n",
    "    generator.add(Dense(512))\n",
    "    generator.add(LeakyReLU(0.5))\n",
    "    generator.add(Dense(1024))\n",
    "    generator.add(LeakyReLU(0.5))\n",
    "    generator.add(Dense(data_dim, activation='tanh'))\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# Discriminator model\n",
    "def create_discriminator():\n",
    "    # FILL THIS IN\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(1024, input_dim=data_dim))\n",
    "    discriminator.add(LeakyReLU(0.5))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.5))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    discriminator.add(Dense(256))\n",
    "    discriminator.add(LeakyReLU(0.5))\n",
    "    discriminator.add(Dense(units=1, activation='sigmoid'))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer = adam_optimizer(), metrics=['accuracy'])\n",
    "    return discriminator\n",
    "\n",
    "# Create adversarial model\n",
    "def create_gan(discriminator, generator):\n",
    "    # FILL THIS IN\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(latent_dim,))\n",
    "    x=generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(gan_input, gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer = gan_optimizer(), metrics = ['accuracy'])\n",
    "    return gan\n",
    "\n",
    "# Creating GAN\n",
    "generator = create_generator()\n",
    "discriminator = create_discriminator()\n",
    "gan = create_gan(discriminator, generator)\n",
    "\n",
    "# Model and training parameters\n",
    "#ASSIGN VALUES TO THE FOLLOWING VARIABLES\n",
    "epochs = 5000\n",
    "batch_size = 128\n",
    "sample_interval = 500\n",
    "\n",
    "# Array to save training history\n",
    "training_meta_data = np.zeros([epochs, 4])\n",
    "\n",
    "# Training the GAN\n",
    "for e in range(1, epochs+1):\n",
    "\n",
    "    # Generate random noise as input\n",
    "    # FILL THIS IN\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    \n",
    "    # Generate fake MNIST images from generated noise\n",
    "    # FILL THIS IN\n",
    "    fake_images = generator.predict(noise)\n",
    "    fake_images = np.reshape(fake_images, (batch_size, 28, 28, 1))\n",
    "\n",
    "    # Get a random set of real MNIST images\n",
    "    # FILL THIS IN\n",
    "    idx= np.random.randint(0, data_train.shape[0], batch_size)\n",
    "    real_images = data_train[idx]\n",
    "\n",
    "    # Concatenate real and fake images into a single array (or batch)\n",
    "    # FILL THIS IN\n",
    "    \n",
    "    data_total = np.concatenate((real_images, fake_images), axis = 0)\n",
    "    data_total = np.reshape(data_total, (-1, data_dim))\n",
    "\n",
    "    # Assign training labels (assign high probability, but not 1, to real images)\n",
    "    # FILL THIS IN\n",
    "    labels_real = 0.98*np.ones((batch_size, 1))\n",
    "    labels_fake = 0.02*np.zeros((batch_size, 1))\n",
    "    labels_dis = np.concatenate((labels_real, labels_fake),axis = 0)\n",
    "\n",
    "    # Allow discriminator parameters to be updated\n",
    "    # FILL THIS IN\n",
    "    discriminator.trainable = True\n",
    "\n",
    "    # Train discriminator on batch of real and fake images. Assign loss and accuracy to variable\n",
    "    # FILL THIS IN\n",
    "    dis_training_meta_data = discriminator.fit(data_total, labels_dis, epochs=epochs, batch_size=batch_size, shuffle=True).history\n",
    "    d_loss = [dis_training_meta_data['loss'][-1], dis_training_meta_data['accuracy'][-1]]\n",
    "\n",
    "    \n",
    "    # Train adversarial model and try to fool discriminator (with incorrect label) \n",
    "    # by generating a new batch of noise and assign them labels of real data\n",
    "    # FILL THIS IN\n",
    "    noise2 = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    labels_generator = np.ones((batch_size, 1))\n",
    "\n",
    "    # Keep discriminator weights constant while training generator\n",
    "    # FILL THIS IN\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # Train GAN (without updating discriminator weights) on new batch of fake images. Assign loss and accuracy to variable\n",
    "    # FILL THIS IN\n",
    "    gan_meta= gan.fit(noise2, labels_generator, epochs=epochs, batch_size=batch_size, shuffle=True).history\n",
    "    gan_loss = [gan_meta['loss'][-1], gan_meta['accuracy'][-1]]\n",
    "\n",
    "    # Save training status\n",
    "    # Discriminator and model loss\n",
    "    training_meta_data[e-1, 0] = d_loss[0]\n",
    "    training_meta_data[e-1, 1] = gan_loss[0]\n",
    "\n",
    "    # Discriminator and model accuracy\n",
    "    training_meta_data[e-1, 2] = d_loss[1]\n",
    "    training_meta_data[e-1, 3] = gan_loss[1]\n",
    "\n",
    "\n",
    "    # If at sample interval, print training status and save samples\n",
    "    if e % sample_interval == 0:\n",
    "      \n",
    "        # Print training status\n",
    "        print(\"Epoch %d\" %e)\n",
    "        log_mesg = \"%d: [Discriminaotr loss: %f, acc: %f]\" % (e, d_loss[0], d_loss[1])\n",
    "        log_mesg = \"%s  [GAN loss: %f, acc: %f]\" % (log_mesg, gan_loss[0], gan_loss[1])\n",
    "        #print(log_mesg)\n",
    "        \n",
    "        # Plot images \n",
    "        r, c = 5, 5\n",
    "\n",
    "        # Create images from the noise (predict the outcome of the noise)\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow((gen_imgs[cnt].reshape(28, 28)), cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqRJEDeIG9mx"
   },
   "outputs": [],
   "source": [
    "# Plot model loss vs epoch\n",
    "#FILL THIS CODE BLOCK IN AND PRODUCE PLOT\n",
    "plt.plot(dis_training_meta_data.history['loss'])\n",
    "plt.plot(gan_meta.history['loss'])\n",
    "plt.title('Discriminator and GAN model loss vs. Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Discrminator', 'GAN'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUJwhntuHJK8"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy vs epoch\n",
    "#FILL THIS CODE BLOCK IN AND PRODUCE PLOT\n",
    "plt.plot(dis_training_meta_data.history['accuracy'])\n",
    "plt.plot(gan_meta.history['accuracy'])\n",
    "plt.title('Discriminator and GAN model accuracy vs. Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Discrminator', 'GAN'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dC_kPLFKHS7c"
   },
   "source": [
    "[4]. Compare and comment on the results of GAN with dropout and without dropout.\n",
    "\n",
    "\n",
    "[5][a]. Comment on importance of hyper-parameter tuning\n",
    "\n",
    "\n",
    "[6]. Answer the following questions:\n",
    "\n",
    "\n",
    "\n",
    "1.   Why does the accuracy of the discriminator remain around 50%? Is this a good trait of the GAN? \n",
    "\n",
    "  ANS: As GAN is based on a minimax game based on an adversarial loss function, the generator and the discriminator are under a never-ending loop of oscillation. when the generator improves enough to fool the discriminator, the discriminator would have around 50% accuracy. At this point, the discriminator cannot give appropriate feedback and rather give random feedback which degrades the generator's performance again. After the generator's performance gets worse , the generator will learn the appropriate features again and this oscillation continues when both dicriminator and generator jointly search for equilibrium. So this is not a good trait of the GAN.\n",
    "\n",
    "\n",
    "2.   How could this model be modified to produce cleaner (less noisy) images? \n",
    "\n",
    "  ANS: 1. modify the loss functions since loss functions provide guidance for model weights to follow in the vast state space, it is important that the functions represent the ultimate goals of optimization problems, such as LSGAN, the least-squares loss function. 2. GAN can also be improved with additional variants on architecture. The Neural Networks are variants of GAN resulting from the insights: Generator capable of having Variational Autoencoder (VAE) architecture and Discriminator having multiple outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ZBFSk1RHX5J"
   },
   "source": [
    "#Part 2: Generating samples using trained generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHaVnENuHcKQ"
   },
   "outputs": [],
   "source": [
    "# Generate ten images from Gaussian noise using the trained generator from Part 1\n",
    "# FILL THIS IN\n",
    "\n",
    "# Re-scale generated images to lie in [0, 1]\n",
    "# FILL THIS IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nak-dm-CIC6r"
   },
   "outputs": [],
   "source": [
    "# Visualize generated noise\n",
    "r, c = 2, 5\n",
    "fig, axs = plt.subplots(r, c)\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        axs[i,j].imshow((noise[cnt].reshape(10, 10)), cmap='gray')\n",
    "        axs[i,j].axis('off')\n",
    "        cnt += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-Jir5ULIITP"
   },
   "outputs": [],
   "source": [
    "# Visualize generated samples\n",
    "r, c = 2, 5\n",
    "fig, axs = plt.subplots(r, c)\n",
    "cnt = 0\n",
    "for i in range(r):\n",
    "    for j in range(c):\n",
    "        axs[i,j].imshow((generated_images[cnt].reshape(28, 28)), cmap='gray')\n",
    "        axs[i,j].axis('off')\n",
    "        cnt += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4I4Q8fhIheQ"
   },
   "source": [
    "#Part 3: Testing accuracy of generated images on ten samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHjnlh6dIiMv"
   },
   "outputs": [],
   "source": [
    "# Load mnist classifier and generated images\n",
    "mnist_classifier = load_model('mnist_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_BFS0cgInWF"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2885888423b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert integer labels to one-hot labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Show classifications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# ASSIGN CLASSES\n",
    "labels = []\n",
    "\n",
    "# Convert integer labels to one-hot labels \n",
    "labels = keras.utils.np_utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Show classifications\n",
    "# FILL THIS IN \n",
    "\n",
    "# Evaluate accuracy\n",
    "# FILL THIS IN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxCLMvJnI95c"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_3_Student.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
